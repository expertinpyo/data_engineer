# Streamlined Data Ingestion with pandas

# Importing Data from Flat Files



파이프 라인이라는 것이

ETL, 즉 데이터 추출, 데이터 변형, 데이터 로드 => 이 과정을 일컫는 말이라고 생각하면 될 듯.



## Importing Data from Flat Files

데이터 수집 based on pandas



### Data Frame

2차원 데이터 구조

columns(열)

Rows(행, index 있음) => 기본 인덱스는 행 번호이지만 열을 인덱스로 지정 가능



### Flat Files

단순. 데이터 저장 및 공유에 널리 사용됨

데이터베이스 관리 시스템 및 스프레드시트에서 내보낼 수 있음

데이터는 일반 텍스트로 저장됨(bold 이런건 없음)

파일의 각 줄은 구분 기호라고 하는 선택된 문자로 구분된 열 값과 함께 하나의 행을 나타냄

일반적으로 구분 기호는 쉼표 => csv 파일 (다른 문자 사용 가능)

단일 pandas 함수는 구분 기호에 관계없이 모든 플랫파일 로드함 `` read_csv(sep='기호(default: 쉼표)')``, 



## Modifying flat file imports

### 데이터 양 제한하기



1. usecols keyword 사용 => 불러올 열 선택하기

​		열 이름 혹은 열 번호로 불러올 수 있음

​		``pd.read_csv('asd.csv', usecols=col_names)``

2. nrows로 가져온 행 수를 줄이는 것

​		``pd.read_csv('asd.csv', nrows=1000)`` => 1000개 가져온다

​		주로 skiprows랑 같이 사용하는 듯,=> 건너 뛸 행 번호 목록을 가져옴





## Handling errors and missing data

### 에러와 누락된 데이터 처리하기

pandas가 데이터 타입을 추측하게 하는 대신 직접 데이터 유형을 설정할 수 있음

=> dtype => dict임, key : 열 이름 / value : 열에 있어야 하는 데이터 유형

(ex, data.dtypes) => 확인할 때 



na_values => 데이터를 로드할 때 해당 데이터는 NA 처리 해줌

추후 isna()로 해당 항목들 확인 가능



error_bad_lines = False면 에러가 나도 생략하고 에러나지 않는 다른 데이터들을 로드함

warn_bad_lines=True

=> 파싱할 수 없는 줄을 건너뛸 떄 메세지 표시 여부를 pandas에 알려준다



# Importing Data From Excel Files

## Introduction to spreadsheets

스프레드시트에 있는 데이터를 활용한 파이프라인 만드는 법에 대한 학습

스프레드시트는 행과 열에 데이터 셀이 있는 테이블에 정보를 구성함(데이터 프레임과 흡사)

스프레드 시트에는 결과가 자동으로 업데이트 될 수도 있음



pandas : read_excel()

norws, skiprows는 csv와 비슷함

usecols는 로드할 열의 정수 혹은 "a:p"같은 excel 열 문자 or 범위 문자열도 허용함

=> usecols에 들어가는 것은 str 형태여야 함.

ex) a, e~j 를 원하는 경우 'a, e:j'



## Getting Data from Multiple worksheets

여러 시트에서 데이터 불러오기

기본적으로 read_excel은 통합문서의 첫 번째 시트에서만 데이터를 불러옴

=> sheet_name 사용시 해당 이름의 시트에서 데이터 불러옴

​	=> 시트 이름 혹은 시트 내임과 읽을 위치 번호의 조합을 포함하는 목록임



여러 시트를 한꺼번에 불러오는 방법

=> sheet_name=None



## Modifying imports : true / false data

Boolean 데이터 다루기

pandas는 열을 boolean로 해석하지 않고 float로 해석함



excel의 dtype으로 수정함, dict()



dtype으로 bool type으로 수정할 시

=> 예/아니오 항목 모두가 True로 나옴

NA check할 시 아무것도 tracking 되지 않음

=> pandas가 0 : False, 1: True로 읽긴 했지만 모두 True로 리턴해버림

=> 이 경우

true_values, false_values(둘 다 리스트)를 설정함으로써 해결할 수 있음



## Modifying imports: parsing dates

datetimes 데이터 유형에 초점 두기

python은 datetime 유형으로 날짜 데이터 저장함



기본적으로 pandas는 object(str)로서 datetime 불러옴

따라서 이 것을 datetime으로 바꿔줘야 하는데

이 때 사용하는 것 => Not dtype But "parsedates"

parse_dates => list, dict임

list in list or dict 하면 날짜와 숫자가 구분되게 나옴



pd.to_datetime(대상, format="원하는 형식")으로 datetime 유형으로 변경 가능



# Importing Data from Databases

## Introduction to databases

관계형 데이터베이스에 대한 파이프라인 구축에 대하여 학습



관계형 데이터베이스 =>  엔티티(실재, 우리가 필요한 정보같은 개념)와 속성 열을 나타내는 행과 함께 테이블에서 엔티티에 대한 데이터를 구성.

데이터 프레임, 플랫 파일 및 많은 Excel 시트가 유사하게 데이터를 정렬함

하지만, 관계형 데이터베이스는 고유한 레코드 식별자 또는 키를 통해 테이블을 연결하거나 관련시킬 수 있다는 점에서 다름

데이터베이스 => 스프레드시트 또는 플랫 파일보다 더 많은 데이터를 처리하고 더 많은 동시 사용자를 지원함.

데이터 타입은 각 열을 통해 구분됨.

특정 언어(구조화된 쿼리 언어 또는 SQL) 를 통해 데이터베이스와 상호 작용함. 



EX) SQL Server, ORACLE, PostgreSQL, SQLite 등



데이터베이스에서 데이터 읽는 방법

1. 데이터 베이스에 연결함
2. SQL 및 pandas로 쿼리한다



이 것을 위해 SQLAlchemy 라이브러리를 사용할 예정



pd.read_sql(query, engine)

query : 불러오려는 sql의 문자열 or 테이블 이름

engine : 데이터베이스에 연결하는 방법, 우리가 만들 예정



## Refining import with SQL queries

pd.read_sql은 excel이나 csv 보다 적은 인수를 가짐

BUT, sql은 더 많은 방법으로 커스터마이즈된 데이터를 import할 수 있게 해줌



열선택, WHERE, 숫자-텍스트로 필터링 가능

sql과 python 결합해서 필터 걸기 가능

AND와 OR 결합조건을 활용할 수도 있음



## More complex SQL queries

DISTINCT / 집계함수(aggregate function/ average, count같은 것) / GROUP BY  

group by 할 때는 집계함수와 같이 구성되어야 함 => 안그러면 정보가 쓸모 없어지는 듯



- SELECT DISTINCT column name FROM table name

- SUM / AVG / MAX / MIN / COUNT 등

  - ex, SELECT AVG(tmax) FROM weather;

- GROUP BY, FROM 이나 WHERE 뒤에

  

## Loading multiple tables with joins



관계형 데이터베이스의 특징 중 하나인 고유한 레코드 식별자(주로 key가 될수도?)를 통해 테이블을 서로 연결할 수 있다(join)는 점.

=> 테이블을 SQL JOIN과 함께 사용자 지정 데이터셋으로 결합할 수 있음



주로 키값을 이용해 조인 하지만, 키를 다른 테이블에 있는 고유한 값으로 바꿀 수 있음



조인할 때 키는 두 테이블 모두에 나타나는 키 값이 있는 레코드만 반환함

key column은 동일한 데이터 타입을 가지고 있어야함.



조인 후 WHERE, 집계함수, GROUP BY 등으로 데이터 필터링 가능



SELECT FROM JOIN WHERE GROUP BY



# Importing JSON Data and Working with API

## Introduction to JSON



JSON => DataFrame과는 다르게 JSON은 표 형식이 아님

=> 더 효율적인 데이터 저장방법임

​	=> WHY? value 값이 없는 경우 NULL값을 갖기 보다 그 자체를 생략할 수 있기 때문.

read_json()

방향 설정하는 orient 인수 존재

dtype 사용 가능



JSON 데이터는 표 형식이 아니기 때문에 pandas는 DataFrame에 데이터를 로드하기 위해 데이터가 어떻게 배열되는지에 대한 가정을 함

자동으로 records랑 column 방향을 감지함.



행 지향 JSON : dict 목록으로 구성되며 각 dict는 테이블 record로 기록됨

열 지향 JSON : 속성 이름을 반복하지 않음으로써 파일 크기를 줄이기 위해 JSON은 열 지향일 수 있음.

 

read_json으로 데이터 로드 추출 => orient 키워드 인수에 값 전달하여 방향 설정



## Introduction to APIs

API : Application Programming Interfaces 

프로그램이 다른 프로그램과 통신할 수 있도록 정의된 방법

API는 프로그래머들이 다른 어플리케이션의 데이터베이스 구조에 대해 알 필요 없이 그 어플리케이션의 데이터를 얻을 수 있도록 함.



requests 라이브러리를 통해 API 가져옴



requests.get(url, params, headers)

- url : 데이터 가져올 url 문자열
- params : 매개변수 이름과 value를 전달하여 커스터마이즈된 api 요청을 보낼 수 있음
- headers : api에서 사용자 인증 키를 요구할 경우 headers를 통해 전달됨



reponse.json()을 통해 result값 도출 가능 => dict 형태임

따라서 pandas가 해당 값을 읽게 하려면 str로 변환해야함

그러므로 pd.DataFrame()을 통해 해당 값을 읽을 수 있게 해줌



## Working with nested JSONs



가져온 중첩된 JSON 데이터 재구성 하기

pandas로 중첩된 JSON을 병합해보기



- pandas.io.json
  - json_normalize() => sep : 구분기호
    - record_path : 중첩 데이터에 대한 속성 문자열 or 문자열 목록을 사용
    - meta : 결과 데이터 프레임에 포함할 상위 수준 속성 목록 사용
    - meta_prefix(메타 접두사): 출처를 명확히 하고 열 이름 중복 X





## Combining multiple datasets



데이터 세트를 결합하기 위한 pandas 메소드



- append()

  - ignore_index => DateFrame이 pandas의 기본 행의 인덱스를 사용하는 경우, 두개의 같은 행이 행1, 행2로 쪼개지는 것을 방지하기 위해 해당 매개변수를 True로 설정해야함

- merge() : 데이터를 결합하는 고유한 방법, pandas의 join 같은 것

  - 데이터프레임이름.merge() + 병합할 데이터 프레임 이름
  - 두 데이터 프레임에서 키 열 이름이 일치하는 경우 on 키워드로 이름 지정 가능
  - 다른 경우 left_on, right_on 을 사용해 첫번째, 두번째 데이터 프레임에서 키를 지정함
  - Key column(키 열)의 데이터 유형이 동일해야함

  
