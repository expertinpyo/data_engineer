# 빅데이터를 지탱하는 기술 정리

저자 : 니시다 케이스케



### CH 1



빅데이터 취급이 어려운 이유

1. 데이터의 분석 방법을 모름
2. 데이터 처리에 수고와 시간이 소요



빅데이터 대표 기술 : Hadoop and NoSQL

- Hadoop 
  - 다수의 컴퓨터에서 대량의 데이터 처리하기 위한 시스템
- NoSQL
  - 빈번한 읽기/쓰기 및 분산 처리가 강점



이 둘을 조합함으로써 NoSQL 데이터베이스에 기록 + Hadoop으로 분산처리하기



데이터 웨어하우스 => 이전의 데이터 분석 방법

지금도 여전히 

가속도적으로 늘어나는 데이터 처리는 Hadoop으로, 

비교적 작거나 중요한 데이터는 데이터 웨어하우스에 넣는 식으로 사용



빅데이터와 스몰 데이터 차이

- 모두 그냥 데이터임. 본질적인 차이없음
- 대량의 데이터의 경우 과거라면 버릴 수 밖에 없었던 데이터였지만, 빅데이터 시대가 도래하고 이들을 핸들링할 수 있게 된 것일 뿐.



데이터 디스커버리

=> 대화형으로 데이터를 시각화하여 가치 있는 정보를 찾으려고 하는 프로세스, 셀프 서비스용 BI라고 불리움

=> 대표적인 것 :Tableau Public



빅데이터 기술이 기존의 데이터 웨어하우스와 다른 점

=> 다수의 분산 시스템을 조합하여 확장성이 뛰어난 데이터 처리 구조를 만든다는 점.



데이터 파이프라인(Data PipeLine)

=> 차례대로 전달해가는 데이터로 구성된 시스템



데이터 전송(Data Transfer)

- 벌크 형
  - 이미 어딘가에 존재하는 데이터를 처리해 추출하는 방법
  - 데이터베이스와 파일 서버 등에서 정기적으로 데이터를 수집하는데 사용
- 스트리밍 형
  - 차례차례로 생성되는 데이터를 끊임없이 계속해서 보내는 방법
  - 모바일 앱, 임베디드 장비 등에서 사용



요즘 추세는 스트리밍 형 방법임.

스트리밍 형으로 받은 데이터를 실시간으로 처리하는 것 => 스트림 처리(stream processing)

장기적 데이터 분석을 위해서는 배치 처리(batch processing)이 선호됨



워크플로(work flow)

=> 전체 데이터 파이프라인의 동작 관리를 위한 것 / 매일 정해진 시간에 배치 처리를 스케쥴대로 실행함. 오류 발생 시 관리자에게 통지하는 목적으로 사용됨



데이터 웨어하우스와 데이터 마트 - 데이터 파이프라인의 기본형

- 데이터 웨어하우스
  - 대량의 데이터를 장기 보존하는 것에 최적화
  - 소량의 데이터를 자주 읽고 쓰는 것에 부적합
  - ETL 프로세스 : 데이터 소스(RDB, 로그 등을 저장하는 파일 서버)에서 Raw Data Extract, 가공(Transform) 후 데이터 웨어하우스에 저장(Load)

- 데이터 마트
  - 데이터 웨어하우스는 주로 중요한 업무를 다루는 경우가 많으므로 함부로 사용해 과부하 주는 행위를 막아야함
  - 데이터 웨어하우스에서 필요한 데이터들만 추출한 후 데이터 마트를 구축
  - BI도구와 조합시키는 형태, 데이터 시각화에 사용됨



데이터 레이크(Data Lake)

=> 모든 데이터를 원래 형태로 축적 후 나중에 그것을 필요에 따라 가공하는 구조

​	 주로 임의의 데이터를 저장할 수 있는 분산 스토리지가 데이터 레이크로 이용된다.



데이터 레이크

- 단순한 Storage. 그것만으로는 데이터 가공 불가능
- 따라서 MapReduce 등의 분산 데이터 처리 기술을 사용해 이를 가공 및 분석할 수 있음

 

애드 혹 분석(ad hoc analysis)

=> 수작업으로 데이터를 집계하는 것

​	 데이터 마트를 만들지 않고 데이터 레이크, 데이터 웨어하우스에 직접 연결하는 경우 많음



시각화에 BI 도구를 이용할 경우 집계속도 향상을 위해 데이터 마트는 필수적임





데이터 분석

- 확증적 데이터 분석(confirmatory data analysis)
  - 통계학적 모델링에 의한 데이터 분석
- 탐색적 데이터 분석(exploratory data analysis)
  - 데이터 시각화 후 분석

